{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão Linear Múltipla\n",
    "\n",
    "\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "Muitos algoritmos de aprendizagem de máquinas utilização métodos de otimização. Esses algoritmos são usados por algoritmos de aprendizado de máquina para encontrar um bom conjunto de parâmetros do modelo, dado um conjunto de dados de treinamento. O algoritmo de otimização mais comum usado na aprendizagem de máquinas é o gradiente descendente estocástico. Neste tutorial, você descobrirá como implementar uma gradiente descendente estocástico para otimizar um algoritmo de regressão linear.\n",
    "\n",
    "Ao final desta aula voce estará apto a:\n",
    "\n",
    "1. Estimar os coeficientes de regressão linear usando a descida gradiente estocástica.\n",
    "2. Fazer previsões para regressão linear multivariada.\n",
    "3. Implementar regressão linear com  gradiente descendente estocástico para fazer previsões em novos dados.\n",
    "\n",
    "### 1.1 Dataset - Seguro de Veículo Sueco\n",
    "\n",
    "Neste tutorial, usaremos o dataset que trata da Qualidade do Vinhos (Wine Quality Data), o qual pode permitir a previsão da qualidade do vinho branco, ajudando o especialista em vinhos na avaliação de qualidade. O RMSE de linha de base do problema é de aproximadamente 0.148 pontos de qualidade. O arquivo winequality-white.csv está disponível no diretório presente. \n",
    "\n",
    "### 1.2 Regressão Linear Multivariada\n",
    "\n",
    "A regressão linear é uma técnica para prever um valor real. Confusamente, esses problemas em que um valor real deve ser previsto são chamados de problemas de regressão. A regressão linear é uma técnica em que uma linha reta é usada para modelar a relação entre valores de entrada e saída. Em mais de duas dimensões, esta linha reta pode ser pensada como um plano ou hiperplano.\n",
    "\n",
    "As previsões são feitas como uma combinação dos valores de entrada para prever o valor de saída. Cada atributo de entrada (x) é ponderado usando um coeficiente (b), e o objetivo do algoritmo de aprendizagem é descobrir um conjunto de coeficientes que resulte em boas previsões (y).\n",
    "\n",
    "\n",
    "![alt text](images/regressao_linear_multipla_modelo.png \"\")\n",
    "                             \n",
    "Os coeficientes podem ser encontrados usando gradiente descendente estocástico.\n",
    "\n",
    "### 1.3 Gradiente Descendente Estocástico\n",
    "\n",
    "Gradiente Descendente é o processo de minimização de uma função seguindo a inclinação ou gradiente dessa função. Na aprendizagem em máquina, podemos usar uma técnica que avalie e atualize os coeficientes de cada iteração chamada gradiente descendente estocástico para minimizar o erro de um modelo em nossos dados de treinamento.\n",
    "\n",
    "A maneira como esse algoritmo de otimização funciona é que cada instância de treinamento é submetida uma de cada vez ao modelo. O modelo faz uma previsão para uma instância de treinamento, o erro é calculado e o modelo é atualizado para reduzir o erro para a próxima previsão. Este processo é repetido para um número fixo de iterações.\n",
    "\n",
    "Esse procedimento pode ser usado para encontrar o conjunto de coeficientes em um modelo que resulte no menor erro para o modelo nos dados de treinamento. Cada iteração, os coeficientes *(b)* são atualizados usando a equação:\n",
    "\n",
    "\n",
    "![alt text](images/coeficiente_b_gradiente.png \"\")\n",
    "\n",
    "\n",
    "Onde *b* é o coeficiente ou o peso a ser otimizado, a *taxa de aprendizado (learning rate)* é uma taxa que você deve configurar (por exemplo, 0,01), o *erro* é o erro de predição para o modelo nos dados de treinamento atribuídos ao peso e *x* é o valor de entrada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tutorial\n",
    "\n",
    "Este tutorial é dividido em 3 partes:\n",
    "\n",
    "1. Fazendo Predições.\n",
    "2. Coeficientes de estimativa. \n",
    "3. Estudo de caso de qualidade do vinho.\n",
    "\n",
    "Isso proporcionará a base que você precisa implementar e aplicar uma regressão linear com descida descendente estocástica em seus próprios problemas de modelagem preditiva.\n",
    "\n",
    "### 2.1 Fazer previsões\n",
    "\n",
    "O primeiro passo é desenvolver uma função que possa fazer previsões. Isso será necessário tanto na avaliação dos valores dos coeficientes do candidato quanto na aplicação do gradiente descendente estocástico. Após o modelo ser finalizado, começaremos a fazer previsões em dados de teste ou novos dados. Abaixo está uma função chamada *predict ()* que prediz um valor de saída para uma linha, dado um conjunto de coeficientes.\n",
    "\n",
    "O primeiro coeficiente é sempre a intercepção, também chamado de viés (em ingles, bias) ou b0, pois é independente e não é responsável por um valor de entrada específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "  yhat = coefficients[0]\n",
    "  for i in range(len(row)-1):\n",
    "    yhat += coefficients[i + 1] * row[i]\n",
    "  return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo\n",
    "\n",
    "Dado conjunto de dados abaixo, será apresentado o uso da função predict confome exemplo que segue.\n",
    "\n",
    "x |  y\n",
    "--| -\n",
    "1 | 1\n",
    "2 | 3\n",
    "4 | 3\n",
    "3 | 2\n",
    "5 | 5\n",
    "\n",
    "Neste exemplo, existe um único valor de entrada (x) e dois valores de coeficiente (b0 e b1). A equação de predição que modelamos para este problema é:\n",
    "*y = b0 + b1 × x *\n",
    "\n",
    "Ou, com os valores de coeficientes específicos, escolhemos à mão como:\n",
    "y = 0,4 + 0,8 * x \n",
    "\n",
    "Ao executar esta função, obtemos previsões razoavelmente próximas da saída esperada (y) valores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Estimativa de Coeficientes \n",
    "\n",
    "Para podermos estimar os valores dos coeficientes para nossos dados de treinamento usando gradiente descendente estocástico precisamos definir dois parâmetros:\n",
    "\n",
    "1. Taxa de aprendizado: usado para limitar a quantidade que cada coeficiente é corrigido sempre que é atualizado.\n",
    "2. Épocas: o número de vezes para percorrer os dados de treinamento ao atualizar os coeficientes.\n",
    "\n",
    "Estes parametros, juntamente com os dados de treinamento, serão os argumentos para a função. Existem 3 loops que precisamos executar na função:\n",
    "\n",
    "1. Loop em cada época.\n",
    "2. Faça um loop sobre cada linha nos dados de treinamento para uma época.\n",
    "3. Faça um loop sobre cada coeficiente e atualize-o para uma linha dos dados em uma época.\n",
    "\n",
    "Como você pode ver, atualizamos cada coeficiente para cada linha nos dados de treinamento, em cada época. Os coeficientes são atualizados com base no erro que o modelo fez. O erro é calculado como a diferença entre a previsão feita com os coeficientes do candidato e o valor de saída esperado.\n",
    "\n",
    "*error = prediction − expected*\n",
    "\n",
    "Existe um coeficiente para ponderar cada atributo de entrada, e estes são atualizados de forma consistente, por exemplo:\n",
    " \n",
    "b1 (t + 1) = b1 (t) - taxa de aprendizado x erro (t) x x1 (t)\n",
    "\n",
    "\n",
    "O coeficiente especial no início da lista, também chamado de intercepção ou a polarização, é atualizado de forma semelhante, exceto sem uma entrada porque não está associado a um valor de entrada específico:\n",
    "\n",
    "b0 (t + 1) = b0 (t) - taxa de aprendizagem * erro (t) \n",
    "\n",
    "Abaixo está uma função denominada coefficients_sgd() que calcula valores de coeficientes para um conjunto de dados de treinamento usando gradiente descendente estocástico.\n",
    "\n",
    "Usamos uma pequena taxa de aprendizado de 0,001 e treinamos o modelo por 50 épocas, ou 50 exposições dos coeficientes para todo o conjunto de dados de treinamento. Ao executar o exemplo, imprime uma mensagem a cada época com o erro de soma quadrada para aquela época e o conjunto final de coeficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    coef = [0.0 for i in range(len(train[0]))]\n",
    "    print ('Coeficiente Inicial=' + str(coef))\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            yhat = predict(row, coef)\n",
    "            error = yhat - row[-1]\n",
    "            sum_error += error**2\n",
    "            coef[0] = coef[0] - l_rate * error\n",
    "            for i in range(len(row)-1):\n",
    "                coef[i + 1] = coef[i + 1] - l_rate * error * row[i] \n",
    "        print(('epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error)))\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    dataset_aux = dataset[:10]\n",
    "    print(dataset_aux)\n",
    "    dataset_values = dataset_aux.values\n",
    "    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(1, 10))\n",
    "    dataset_scaled = min_max_scaler.fit_transform(dataset_values)\n",
    "    df = pd.DataFrame(dataset_scaled)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.70</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.90</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.045</td>\n",
       "      <td>30.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.9949</td>\n",
       "      <td>3.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>9.6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.70</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.044</td>\n",
       "      <td>28.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.45</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.033</td>\n",
       "      <td>11.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.56</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0             7.0              0.27         0.36           20.70      0.045   \n",
       "1             6.3              0.30         0.34            1.60      0.049   \n",
       "2             8.1              0.28         0.40            6.90      0.050   \n",
       "3             7.2              0.23         0.32            8.50      0.058   \n",
       "4             7.2              0.23         0.32            8.50      0.058   \n",
       "5             8.1              0.28         0.40            6.90      0.050   \n",
       "6             6.2              0.32         0.16            7.00      0.045   \n",
       "7             7.0              0.27         0.36           20.70      0.045   \n",
       "8             6.3              0.30         0.34            1.60      0.049   \n",
       "9             8.1              0.22         0.43            1.50      0.044   \n",
       "10            8.1              0.27         0.41            1.45      0.033   \n",
       "\n",
       "    free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                  45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                  14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                  30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                  47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                  47.0                 186.0   0.9956  3.19       0.40   \n",
       "5                  30.0                  97.0   0.9951  3.26       0.44   \n",
       "6                  30.0                 136.0   0.9949  3.18       0.47   \n",
       "7                  45.0                 170.0   1.0010  3.00       0.45   \n",
       "8                  14.0                 132.0   0.9940  3.30       0.49   \n",
       "9                  28.0                 129.0   0.9938  3.22       0.45   \n",
       "10                 11.0                  63.0   0.9908  2.99       0.56   \n",
       "\n",
       "    alcohol  quality  \n",
       "0       8.8        6  \n",
       "1       9.5        6  \n",
       "2      10.1        6  \n",
       "3       9.9        6  \n",
       "4       9.9        6  \n",
       "5      10.1        6  \n",
       "6       9.6        6  \n",
       "7       8.8        6  \n",
       "8       9.5        6  \n",
       "9      11.0        6  \n",
       "10     12.0        5  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"winequality-white.csv\", delimiter=\";\")\n",
    "dataset.loc[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0     1          2          3          4          5          6   \\\n",
      "0   4.789474   5.5   7.666667  10.000000   1.642857   9.454545   8.382022   \n",
      "1   1.473684   8.2   7.000000   1.046875   4.214286   1.000000   4.539326   \n",
      "2  10.000000   6.4   9.000000   3.531250   4.857143   5.363636   1.000000   \n",
      "3   5.736842   1.9   6.333333   4.281250  10.000000  10.000000  10.000000   \n",
      "4   5.736842   1.9   6.333333   4.281250  10.000000  10.000000  10.000000   \n",
      "5  10.000000   6.4   9.000000   3.531250   4.857143   5.363636   1.000000   \n",
      "6   1.000000  10.0   1.000000   3.578125   1.642857   5.363636   4.943820   \n",
      "7   4.789474   5.5   7.666667  10.000000   1.642857   9.454545   8.382022   \n",
      "8   1.473684   8.2   7.000000   1.046875   4.214286   1.000000   4.539326   \n",
      "9  10.000000   1.0  10.000000   1.000000   1.000000   4.818182   4.235955   \n",
      "\n",
      "       7     8     9          10   11  \n",
      "0  10.000   1.0   6.0   1.000000  1.0  \n",
      "1   1.250  10.0  10.0   3.863636  1.0  \n",
      "2   2.625   8.8   5.0   6.318182  1.0  \n",
      "3   3.250   6.7   1.0   5.500000  1.0  \n",
      "4   3.250   6.7   1.0   5.500000  1.0  \n",
      "5   2.625   8.8   5.0   6.318182  1.0  \n",
      "6   2.375   6.4   8.0   4.272727  1.0  \n",
      "7  10.000   1.0   6.0   1.000000  1.0  \n",
      "8   1.250  10.0  10.0   3.863636  1.0  \n",
      "9   1.000   7.6   6.0  10.000000  1.0  \n"
     ]
    }
   ],
   "source": [
    "df = normalize(train_dataset)\n",
    "train_dataset = df.head(int(len(dataset)*0.8))\n",
    "test_dataset = df.tail(int(len(dataset)*0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rate = 0.001\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficiente Inicial=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "epoch=0, lrate=0.001, error=7171.804\n",
      "epoch=1, lrate=0.001, error=6833.875\n",
      "epoch=2, lrate=0.001, error=6794.795\n",
      "epoch=3, lrate=0.001, error=6757.905\n",
      "epoch=4, lrate=0.001, error=6723.087\n",
      "epoch=5, lrate=0.001, error=6690.762\n",
      "epoch=6, lrate=0.001, error=6660.884\n",
      "epoch=7, lrate=0.001, error=6633.275\n",
      "epoch=8, lrate=0.001, error=6607.735\n",
      "epoch=9, lrate=0.001, error=6584.075\n",
      "epoch=10, lrate=0.001, error=6562.123\n",
      "epoch=11, lrate=0.001, error=6541.726\n",
      "epoch=12, lrate=0.001, error=6522.745\n",
      "epoch=13, lrate=0.001, error=6505.058\n",
      "epoch=14, lrate=0.001, error=6488.555\n",
      "epoch=15, lrate=0.001, error=6473.135\n",
      "epoch=16, lrate=0.001, error=6458.709\n",
      "epoch=17, lrate=0.001, error=6445.199\n",
      "epoch=18, lrate=0.001, error=6432.531\n",
      "epoch=19, lrate=0.001, error=6420.640\n",
      "epoch=20, lrate=0.001, error=6409.468\n",
      "epoch=21, lrate=0.001, error=6398.961\n",
      "epoch=22, lrate=0.001, error=6389.070\n",
      "epoch=23, lrate=0.001, error=6379.752\n",
      "epoch=24, lrate=0.001, error=6370.966\n",
      "epoch=25, lrate=0.001, error=6362.676\n",
      "epoch=26, lrate=0.001, error=6354.849\n",
      "epoch=27, lrate=0.001, error=6347.453\n",
      "epoch=28, lrate=0.001, error=6340.462\n",
      "epoch=29, lrate=0.001, error=6333.848\n",
      "epoch=30, lrate=0.001, error=6327.589\n",
      "epoch=31, lrate=0.001, error=6321.662\n",
      "epoch=32, lrate=0.001, error=6316.048\n",
      "epoch=33, lrate=0.001, error=6310.727\n",
      "epoch=34, lrate=0.001, error=6305.683\n",
      "epoch=35, lrate=0.001, error=6300.899\n",
      "epoch=36, lrate=0.001, error=6296.360\n",
      "epoch=37, lrate=0.001, error=6292.053\n",
      "epoch=38, lrate=0.001, error=6287.964\n",
      "epoch=39, lrate=0.001, error=6284.081\n",
      "epoch=40, lrate=0.001, error=6280.394\n",
      "epoch=41, lrate=0.001, error=6276.891\n",
      "epoch=42, lrate=0.001, error=6273.563\n",
      "epoch=43, lrate=0.001, error=6270.400\n",
      "epoch=44, lrate=0.001, error=6267.394\n",
      "epoch=45, lrate=0.001, error=6264.536\n",
      "epoch=46, lrate=0.001, error=6261.819\n",
      "epoch=47, lrate=0.001, error=6259.235\n",
      "epoch=48, lrate=0.001, error=6256.778\n",
      "epoch=49, lrate=0.001, error=6254.441\n",
      "epoch=50, lrate=0.001, error=6252.218\n",
      "epoch=51, lrate=0.001, error=6250.104\n",
      "epoch=52, lrate=0.001, error=6248.092\n",
      "epoch=53, lrate=0.001, error=6246.178\n",
      "epoch=54, lrate=0.001, error=6244.357\n",
      "epoch=55, lrate=0.001, error=6242.624\n",
      "epoch=56, lrate=0.001, error=6240.975\n",
      "epoch=57, lrate=0.001, error=6239.405\n",
      "epoch=58, lrate=0.001, error=6237.911\n",
      "epoch=59, lrate=0.001, error=6236.490\n",
      "epoch=60, lrate=0.001, error=6235.137\n",
      "epoch=61, lrate=0.001, error=6233.849\n",
      "epoch=62, lrate=0.001, error=6232.623\n",
      "epoch=63, lrate=0.001, error=6231.457\n",
      "epoch=64, lrate=0.001, error=6230.346\n",
      "epoch=65, lrate=0.001, error=6229.289\n",
      "epoch=66, lrate=0.001, error=6228.283\n",
      "epoch=67, lrate=0.001, error=6227.325\n",
      "epoch=68, lrate=0.001, error=6226.413\n",
      "epoch=69, lrate=0.001, error=6225.545\n",
      "epoch=70, lrate=0.001, error=6224.719\n",
      "epoch=71, lrate=0.001, error=6223.932\n",
      "epoch=72, lrate=0.001, error=6223.183\n",
      "epoch=73, lrate=0.001, error=6222.470\n",
      "epoch=74, lrate=0.001, error=6221.792\n",
      "epoch=75, lrate=0.001, error=6221.146\n",
      "epoch=76, lrate=0.001, error=6220.531\n",
      "epoch=77, lrate=0.001, error=6219.946\n",
      "epoch=78, lrate=0.001, error=6219.388\n",
      "epoch=79, lrate=0.001, error=6218.858\n",
      "epoch=80, lrate=0.001, error=6218.353\n",
      "epoch=81, lrate=0.001, error=6217.872\n",
      "epoch=82, lrate=0.001, error=6217.415\n",
      "epoch=83, lrate=0.001, error=6216.979\n",
      "epoch=84, lrate=0.001, error=6216.565\n",
      "epoch=85, lrate=0.001, error=6216.170\n",
      "epoch=86, lrate=0.001, error=6215.794\n",
      "epoch=87, lrate=0.001, error=6215.437\n",
      "epoch=88, lrate=0.001, error=6215.096\n",
      "epoch=89, lrate=0.001, error=6214.772\n",
      "epoch=90, lrate=0.001, error=6214.464\n",
      "epoch=91, lrate=0.001, error=6214.171\n",
      "epoch=92, lrate=0.001, error=6213.891\n",
      "epoch=93, lrate=0.001, error=6213.625\n",
      "epoch=94, lrate=0.001, error=6213.372\n",
      "epoch=95, lrate=0.001, error=6213.132\n",
      "epoch=96, lrate=0.001, error=6212.902\n",
      "epoch=97, lrate=0.001, error=6212.684\n",
      "epoch=98, lrate=0.001, error=6212.477\n",
      "epoch=99, lrate=0.001, error=6212.279\n",
      "[4.098232750519177, 0.003584560029344759, -0.2471710361708433, -0.02352384187868588, 0.9188367685021647, 0.03666675190386227, 0.1498857921109081, -0.005615732525621057, -1.2007620486633028, 0.06414339768935107, 0.11574158768450866, 0.2276074008119333]\n"
     ]
    }
   ],
   "source": [
    "coeff = coefficients_sgd(train_dataset.values, l_rate, n_epoch)\n",
    "print(coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
